{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gerson\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import pickle as pickle\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from math import log10\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#implementação da função de carregamento de textos \n",
    "def loadCorpus (sourcepath):\n",
    "    corpus ={}\n",
    "    for filename in os.listdir (sourcepath):\n",
    "        fh = open(os.path.join(sourcepath,filename), 'r')\n",
    "        #dicionario indexado pelo nome do arquivo que associa cada documento\n",
    "        #a lista de linhas do seu texto\n",
    "        corpus[filename] = fh.readlines()\n",
    "        fh.close()\n",
    "    return corpus\n",
    "\n",
    "def processCorpus(corpus , param_foldCase, param_language, param_stopWords,param_stemmer ):\n",
    "    newCorpus = {}\n",
    "    for document in corpus:\n",
    "        content = []\n",
    "        for sentence in corpus[document]:\n",
    "            sentence = sentence.rstrip('\\n')\n",
    "            sentence = foldCase(sentence,param_foldCase)\n",
    "            listOfTokens = tokenize(sentence)\n",
    "            listOfTokens = removeStopWords(listOfTokens,param_stopWords)\n",
    "            listOfTokens = applyStemming (listOfTokens,param_stemmer)\n",
    "            content.append(listOfTokens) #após os tratamentos acima a setença é inserida no array content \n",
    "        newCorpus[document] = content\n",
    "    return newCorpus \n",
    "\n",
    "def foldCase (line , parameter ):\n",
    "    if(parameter) : line  = line.lower()\n",
    "    return line\n",
    "\n",
    "def tokenize (line):\n",
    "    line = line.replace(\"_\", \" \")\n",
    "    regExpr = '\\W+'\n",
    "    return filter(None,re.split(regExpr,line))\n",
    "\n",
    "def removeStopWords(listOfTokens, listOfStopWords):\n",
    "    return [token for token in listOfTokens if token not in listOfStopWords]    \n",
    "\n",
    "def applyStemming(listOfTokens, stemmer):\n",
    "    return [stemmer.stem(token) for token in listOfTokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tratamentoTexto (dicTexto):\n",
    "    corpus = {}\n",
    "    TextoAux = []\n",
    "    n = 0\n",
    "    contatorTextos = 0 \n",
    "    for texto in dicTexto:\n",
    "        for linha in dicTexto[texto]:\n",
    "            TextoAux.append (linha)\n",
    "            if (re.match(\"From:\",linha ) != None): #verifica se a linha começa com coma tag \"From:\"\n",
    "                n += 1\n",
    "                if (n == 2):\n",
    "                    contatorTextos +=1\n",
    "                    corpus[texto+str(contatorTextos)]= TextoAux.copy()\n",
    "                    TextoAux.clear()\n",
    "                    n=0              \n",
    "                    if(contatorTextos ==5001):\n",
    "                        return corpus\n",
    "        contatorTextos +=1\n",
    "        corpus[texto+str(contatorTextos)]= TextoAux.copy()\n",
    "        TextoAux.clear()\n",
    "        n=0\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    d[1] = 'era uma casa muito engracada'\n",
    "    t2 = ['era', 'uma', 'casa', 'muito', 'engracada']\n",
    "    \n",
    "    d[2] = 'era uma vez'\n",
    "    t2 = ['era', 'uma', 'vez']\n",
    "    \n",
    "    # binary\n",
    "    features = t = ['era' 'uma' 'casa' 'muito' 'engracada' 'vez'] # todas as palavras\n",
    "    rowsBinary = \n",
    "    [\n",
    "        [1, 1, 1, 1, 0],\n",
    "        [1, 1, 0, 0, 1]\n",
    "    ] \n",
    "    \n",
    "    rowsTf = [\n",
    "        [1/5]\n",
    "    ]\n",
    "    \n",
    "    rowsTfIdf = [\n",
    "        [1/5]\n",
    "    ]\n",
    "    \n",
    "    # matriz\n",
    "    rows x features\n",
    "    \n",
    "    2000 x 82100\n",
    "    tf > 3\n",
    "    tfidf > 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#>>>>>>>>>>>>>>>>>>>>>>função que prepara o estágio de representação dos dados <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "def representCorpus(corpus, perct):\n",
    "    print(\"entrou na representação\")\n",
    "    newCorpus = {}\n",
    "    #neste ponto é gerado um único vetor de tokens relacionado a cada documento   \n",
    "    for document in corpus:\n",
    "        newCorpus[document] = [token for sentence in corpus[document] for token in sentence]\n",
    "\n",
    "    #cria uma lista com todos os tokens  que ocorrem em cada texto considerado.\n",
    "    allTokens  = []\n",
    "    for document in newCorpus:\n",
    "        allTokens = allTokens + list(set(newCorpus[document]))\n",
    "    \n",
    "    allTokensDistinct = sorted(list(set(allTokens)))\n",
    "\n",
    "    itensidadeToken = RefinamentoTokens(newCorpus,allTokensDistinct,perct)\n",
    "    \n",
    "    serialise(itensidadeToken, \"vetorTokens\")\n",
    "\n",
    "    RepresetationBinary(newCorpus,itensidadeToken )\n",
    "    RepresetationTF(newCorpus, itensidadeToken)\n",
    "\n",
    "    #cria o dicionárido reverso, nesta etapa é contabilizado quantas vezes um determinado token aparecem no conjunto total de textos\n",
    "    idfDict = {}\n",
    "    for token in allTokens:\n",
    "        try :\n",
    "            idfDict[token] +=1\n",
    "        except KeyError:\n",
    "            idfDict[token] = 1 \n",
    "        \n",
    "   #armazena a quantidade de textos considerados\n",
    "    nDocument = len(corpus)\n",
    "    print (\"Quantidade de textos usados :\" + str(nDocument))\n",
    "    \n",
    " # atualiza o dicionario reverso, associando cada token com seu idf score\n",
    "    for token in idfDict:\n",
    "         idfDict[token] =log10(nDocument/float(idfDict[token]))    \n",
    "    \n",
    "     #computa a matriz termo-documento (newCorpus)\n",
    "    for document in newCorpus:\n",
    "        #computa um dicionario com os if scores de cada termo que ocorre no documento \n",
    "        dictOfTfScoredTokens = tf(newCorpus[document])\n",
    "\n",
    "        #computa um dicionário com o tf-idf score de cada par termo-documento\n",
    "        newCorpus[document] = ({token : dictOfTfScoredTokens[token] *idfDict[token] for token in dictOfTfScoredTokens })\n",
    "    \n",
    "    RepresetationTfIdf(newCorpus, itensidadeToken)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RefinamentoTokens(corpus , listTokens, perctRefinamento):\n",
    "    ListIntensidade = {}\n",
    "    vetorAux = []\n",
    "    qtdTokenRefinado = 0\n",
    "    lista = []\n",
    "\n",
    "    for document in corpus:\n",
    "        vetorAux = list(set(corpus[document]))        \n",
    "        for token  in vetorAux:\n",
    "            if (token in ListIntensidade.keys()) : ListIntensidade[token] += 1\n",
    "            else:                       ListIntensidade[token] = 1\n",
    "        \n",
    "    qtdTokenRefinado  = int(len(ListIntensidade) * perctRefinamento)\n",
    "\n",
    "    ListIntensidade = sorted(ListIntensidade.items(), key=operator.itemgetter(1),reverse=True)   \n",
    "    ListIntensidade = (ListIntensidade [0:qtdTokenRefinado])\n",
    "\n",
    "    for chave in ListIntensidade:\n",
    "        lista.append(chave [0])\n",
    "    return lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RepresetationBinary(corpus,ListIntensidade):\n",
    "    vetorAux =[]\n",
    "    mBinary2 = []\n",
    "\n",
    "    for document in corpus:\n",
    "        vetorAux = list(set(corpus[document]))\n",
    "        mBinary1 = [0]* len(ListIntensidade)\n",
    "        \n",
    "        for token  in vetorAux:       \n",
    "            if token in ListIntensidade: # Verificar se o token pertence ao grupo de tokens com maior frequência no conjunto de texto(ListIntensidade)\n",
    "                mBinary1[ListIntensidade.index(token)] = 1                          \n",
    "        mBinary2.append(mBinary1)\n",
    "    serialise(mBinary2,\"matrizBinaria\") #armazena a matriz binária (mBinary2)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RepresetationTF (corpus,ListIntensidade):\n",
    "    dicAux ={}\n",
    "    mTf2 = []\n",
    "\n",
    "    for document in corpus:\n",
    "        mTf1 = [0]* len(ListIntensidade)\n",
    "        dicAux = tf(corpus[document])\n",
    "        for token in dicAux:\n",
    "            if token in ListIntensidade: # Verificar se o token pertence ao grupo de tokens com maior frequência no conjunto de texto(ListIntensidade)\n",
    "                mTf1[ListIntensidade.index(token)]  = dicAux[token]\n",
    "        mTf2.append(mTf1)\n",
    "    serialise(mTf2,\"matrizTF\") #armazena a matriz TF (mTf2)\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RepresetationTfIdf (corpus, ListIntensidade):\n",
    "    dicAux ={}\n",
    "    mTfIdf2 = []\n",
    "    for document in corpus:\n",
    "        mTfIdf1 = [0]* len(ListIntensidade)\n",
    "        dicAux = corpus[document]\n",
    "        for token in dicAux:\n",
    "            if token in ListIntensidade: # Verificar se o token pertence ao grupo de tokens com maior frequência no conjunto de texto(ListIntensidade)\n",
    "                mTfIdf1[ListIntensidade.index(token)]  = dicAux[token]\n",
    "        mTfIdf2.append(mTfIdf1)\n",
    "    serialise(mTfIdf2,\"matrizTfIdf\") #armazena a matriz Tf-Idf (mTfIdf2)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialise (obj,name):\n",
    "    f=open(name+ '.pkl', 'wb')\n",
    "    p = pickle.Pickler(f)\n",
    "    p.fast= True\n",
    "    p.dump(obj)\n",
    "    f.close()\n",
    "    p.clear_memo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(listOfTokens):\n",
    "    # cria um dicionario associando cada token com o numero de vezes\n",
    "    # em que ele ocorre no documento (cujo conteudo eh listOfTokens)\n",
    "    types = {}\n",
    "    for token in listOfTokens:\n",
    "        if (token in types.keys()) : types[token] += 1\n",
    "        else:                       types[token] = 1\n",
    "    return types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sourcepath = './texts'\n",
    "    #invocar a função de carregamento de textos\n",
    "    corpus1 = loadCorpus(sourcepath)\n",
    "    percentual = 0.75 ## aqui é estabelecido qual o percentual de tokens que deve ser mantido, exluindo os tokens com menor representação no conjunto de textos\n",
    "\n",
    "    textosTratados = tratamentoTexto(corpus1) #neste passo é feito a delimitação dos textos que estão em cada arquivo txt\n",
    "\n",
    "    #definindo as variáveis para o pré-processamento\n",
    "    _foldCase = True\n",
    "    _language = 'english'\n",
    "    _stopWords = stopwords.words('english')\n",
    "    _stemmer = SnowballStemmer(_language)\n",
    "\n",
    "    #aqui ocorre o pré processamento ,Tokenização Remoção de stop words, case-folding , redução para o radical e lematização\n",
    "    corpus2 = processCorpus(textosTratados,_foldCase,_language,_stopWords,_stemmer)\n",
    "\n",
    "    #início da representação de dados \n",
    "    representCorpus (corpus2,percentual)\n",
    "\n",
    "    print(\"Process finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entrou na representação\n",
      "Quantidade de textos usados :1837\n",
      "Process finished\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf(all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
