{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ACH2016 - Inteligência Artificial\n",
    "\n",
    "### Rubens Victor Gomes 8598878 <br> Higor 22222222 ### \n",
    "\n",
    "[Kmeans](#kmeans)<br>\n",
    "Kmeans ++<br>\n",
    "Small dataset pre processing<br>\n",
    "Binary<br>\n",
    "Tf-Df<br>\n",
    "Tf-Idf<br>\n",
    "Big dataset pre processing<br>\n",
    "[Visualization of the clusters](#visualization)<br>\n",
    "[Silhouette visualization](#visualization) <br>\n",
    "Silhouette implementation<br>\n",
    "Elbow Method<br>\n",
    "IEEE Report<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/rgomes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "#Importing the dependencies\n",
    "import re, string, unicodedata, itertools, random, os, glob, math\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from scipy.spatial import distance_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "print(__doc__)\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "plt.rcParams['figure.figsize'] = (24, 9)\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading...\n",
      "loaded\n",
      "(1255, 12187)\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# LOAD THE DATASET\n",
    "print('reading...')\n",
    "X = np.loadtxt('tf_matrix_normalized.txt')\n",
    "print('loaded')\n",
    "print(X.shape)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Centroids initializations\n",
    "# @method\n",
    "# 'zeros' initialize with zeros\n",
    "# 'random' pick k random datapoints\n",
    "# 'k-means++' pick the first random, then pick k - 1 datapoints that maximizes the D² between them\n",
    "def initializeCentroid(X, K, method):\n",
    "    centroids = []\n",
    "        \n",
    "    # maxime the squared distance of centroids\n",
    "    # 1.Choose one center uniformly at random from among the data points.\n",
    "    # 2.For each data point x, compute D(x), the distance between x and the nearest center that has already been chosen.\n",
    "    # 3.Choose one new data point at random as a new center, \n",
    "    # using a weighted probability distribution where a point x is chosen with probability proportional to D(x)2.\n",
    "    # 4.Repeat Steps 2 and 3 until k centers have been chosen.\n",
    "    if method == 'k-means++':\n",
    "        #primeiro centroide (aleatorio)\n",
    "        centroids = np.zeros((K, X.shape[1]))\n",
    "        centroids[0] = X[np.random.choice(X.shape[0], 1)]\n",
    "        \n",
    "        num_instances, num_features = X.shape\n",
    "        \n",
    "        #enquanto nao tiver K centroides\n",
    "        for z in range(1,K):\n",
    "            D2 = []\n",
    "            prob = []\n",
    "            cumprob = []\n",
    "        \n",
    "            #calcula matriz de distancia\n",
    "            dist_matrix = distance_matrix(X, centroids)\n",
    "        \n",
    "            for i in np.arange(num_instances):\n",
    "                # finding the closest centroid\n",
    "                d = dist_matrix[i]\n",
    "                D2.append((np.where(d == np.min(d)))[0][0])\n",
    "        \n",
    "            #faz a soma das probabilidades\n",
    "            for i in np.arange(num_instances):\n",
    "                prob.append(D2[i]/sum(D2))\n",
    "        \n",
    "            #gera a probabilidade acumulada e escolhe o centroide\n",
    "            for i in np.arange(num_instances):\n",
    "                somacum = 0\n",
    "                for j in range(0,i):\n",
    "                    somacum += prob[j]\n",
    "                cumprob.append(somacum)\n",
    "                \n",
    "            rn = random.random()\n",
    "            ind = 0\n",
    "            for i in np.arange(num_instances):\n",
    "                if(cumprob[i] >= rn):\n",
    "                    ind = i\n",
    "                    break\n",
    "                \n",
    "            centroids[z] = X[ind]\n",
    "            \n",
    "    if method == 'random':\n",
    "        centroids = X[np.random.choice(X.shape[0], K)]\n",
    "        \n",
    "    return centroids\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KMeans** (__iterations__ :\n",
    "Max number of iterations, _default 300_, \n",
    "__distance__,\n",
    "Type of distance metric, _default euclidian_\n",
    "__method__,\n",
    "Initialization method: 'random', 'k-means++')\n",
    "<a id=\"kmeans\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closest_centroid(point, centroids):\n",
    "    min_distance = float('inf')\n",
    "    belongs_to_cluster = None\n",
    "    for j, centroid in enumerate(centroids):\n",
    "        dist = np.sqrt(np.sum((point-centroid)**2))\n",
    "        if dist < min_distance:\n",
    "            min_distance = dist\n",
    "            belongs_to_cluster = j\n",
    "\n",
    "    return belongs_to_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equals(centroids_old, centroids_new):\n",
    "    for i in range (len(centroids_old)):\n",
    "        point1 = centroids_old[i]\n",
    "        centroids_new = centroids_new[i]\n",
    "        if (centroids_old != centroids_new).any():\n",
    "            return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeans(X, K, max_iterations, method):\n",
    "    \n",
    "    num_instances, num_features = X.shape\n",
    "    \n",
    "    # Initialize the centroids\n",
    "    # Pick k random points to use as our initial centroids    \n",
    "    initial_centroids = initializeCentroid(X,K,method)\n",
    "    \n",
    "    centroids = initial_centroids\n",
    "    \n",
    "    centroids_old = np.zeros(centroids.shape)\n",
    "    \n",
    "    # this may cause the centroids start on a good stop and converge faster\n",
    "    # so its important to run 30 times and take the avg\n",
    "    clusters = [initializeCentroid(X,1, 'random') for i in range(K)]\n",
    "    \n",
    "    cluster_labels = np.zeros(num_instances)\n",
    "    \n",
    "    iterations = 1\n",
    "    while not equals(centroids_old, centroids) and iterations < max_iterations:\n",
    "        # make the new centroids the current\n",
    "        centroids_old = centroids.copy()\n",
    "        \n",
    "        # for each point in X\n",
    "        for idx in np.arange(num_instances):\n",
    "        \n",
    "            # assign to the closest centroid\n",
    "            label = closest_centroid(X[idx], centroids)\n",
    "            np.append(X[idx], clusters[label])\n",
    "            cluster_labels[idx] = label\n",
    "\n",
    "        centroids = [np.mean(cluster, axis=0) for cluster in clusters]\n",
    "        iterations += 1\n",
    "\n",
    "    return np.array(centroids), np.array(cluster_labels), iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette(X,clusters):\n",
    "    dist_matrix = distance_matrix(X,X)\n",
    "    num_instances = X.shape[0]\n",
    "    print(num_instances)\n",
    "    a_array = np.zeros(num_instances)\n",
    "    b_array = np.zeros(num_instances)\n",
    "    b_array += np.inf\n",
    "    groups_number = np.unique(clusters)\n",
    "    num_clusters = groups_number.shape[0]\n",
    "\n",
    "    for i in groups_number:\n",
    "        group_filter = clusters == i\n",
    "        group_matrix = dist_matrix[group_filter,:]\n",
    "        group_matrix = group_matrix[:,group_filter]\n",
    "        a_array[group_filter] = np.sum(group_matrix,axis=1)/(group_matrix[:,0].size-1)\n",
    "\n",
    "        for c in groups_number:\n",
    "            if c != i:\n",
    "                neighbor_group_filter = clusters == c\n",
    "                neighbor_group_matrix = dist_matrix[group_filter,:]\n",
    "                neighbor_group_matrix = neighbor_group_matrix[:,neighbor_group_filter]\n",
    "                neighbor_distances_array = np.mean(neighbor_group_matrix,axis=1)\n",
    "                b_array[group_filter] = np.minimum(b_array[group_filter], neighbor_distances_array)\n",
    "    \n",
    "    silhouette_array = b_array - a_array\n",
    "    silhouette_array /= np.maximum(a_array,b_array)\n",
    "    return np.mean(silhouette_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET (1255, 12187)\n",
      "took 100 iterations\n",
      "My KMeans\n",
      "For n_clusters = 10 The average silhouette_score is : 0.01644306191110508\n",
      "took 100 iterations\n",
      "My KMeans\n",
      "For n_clusters = 20 The average silhouette_score is : 0.022938623436035744\n"
     ]
    }
   ],
   "source": [
    "print('DATASET', X.shape)\n",
    "\n",
    "range_n_clusters = [10, 20, 50]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax3, ax4) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "    \n",
    "    #-------------------------------------------------------------------------------------------------------------#\n",
    "    \n",
    "     # My kMeans implementation\n",
    "    centroids, cluster_labels, iterations = kMeans(X, K=n_clusters, max_iterations=3, method='k-means++')\n",
    "    \n",
    "    print('took', iterations, 'iterations')\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    #silhouette_avg = silhouette(X, cluster_labels)\n",
    " \n",
    "\n",
    "    print(\"My KMeans\")\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "    \n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / n_clusters)\n",
    "        ax3.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax3.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax3.set_title(\"My Silhouete\")\n",
    "    ax3.set_xlabel(\"Coefficient\")\n",
    "    ax3.set_ylabel(\"Cluster\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax3.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax3.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax3.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "    \n",
    "\n",
    "    # ----------------- My clusters --------------------------------\n",
    "\n",
    "    # REDUCE TO 2 COMPONENTS\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(X)\n",
    "    pca_score = pca.explained_variance_ratio_\n",
    "    Y = pca.fit_transform(X)\n",
    "\n",
    "    colors = cm.spectral(len(cluster_labels) / n_clusters)\n",
    "\n",
    "    ax4.scatter(Y[:, 0], Y[:,1], s = 100, c = cluster_labels)\n",
    "    ax4.scatter(centroids[:, 0], centroids[:, 1], s = 10, c = 'red',label = 'Centroids')\n",
    "\n",
    "    ax4.set_title(\"My Clusters\")\n",
    "    ax4.set_xlabel(\"Feature 1\")\n",
    "    ax4.set_ylabel(\"Feature 2\")   \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
